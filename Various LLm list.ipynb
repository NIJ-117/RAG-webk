{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ec0b015",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Justin Bieber was born on March 1, 1994. The Super Bowl for the 1993 NFL season was Super Bowl XXVIII, which took place on January 30, 1994. In that game, the Dallas Cowboys defeated the Buffalo Bills by a score of 30-13 to win the Super Bowl. Therefore, the Dallas Cowboys were the Super Bowl champions in the year Justin Bieber was born.', response_metadata={'token_usage': {'completion_tokens': 85, 'prompt_tokens': 34, 'total_tokens': 119}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-db9297b9-0e97-46f1-a635-be6a5be92146-0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#OpenAI\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm_chain = prompt | llm\n",
    "question = \"What NFL team won the Super Bowl in the year Justin Beiber was born?\"\n",
    "llm_chain.invoke(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01e6f3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the realm of code, where data flows,\n",
      "A tool arose, LangChain, it grows.\n",
      "A chain of thought, a symphony of code,\n",
      "To unlock knowledge, where language is bestowed.\n",
      "\n",
      "(Chorus)\n",
      "LangChain, LangChain, a weaver of words,\n",
      "Connecting models, like singing birds.\n",
      "From prompts to answers, a seamless flow,\n",
      "Unlocking insights, where knowledge can grow.\n",
      "\n",
      "With chains of prompts, it guides the way,\n",
      "To LLM's power, it shows the day.\n",
      "From simple questions to complex tasks,\n",
      "LangChain delivers, with no time to ask.\n",
      "\n",
      "(Chorus)\n",
      "LangChain, LangChain, a weaver of words,\n",
      "Connecting models, like singing birds.\n",
      "From prompts to answers, a seamless flow,\n",
      "Unlocking insights, where knowledge can grow.\n",
      "\n",
      "With memory and context, it holds the key,\n",
      "To understanding, for you and for me.\n",
      "From chatbots to assistants, it's a versatile tool,\n",
      "Empowering users, making them cool.\n",
      "\n",
      "(Chorus)\n",
      "LangChain, LangChain, a weaver of words,\n",
      "Connecting models, like singing birds.\n",
      "From prompts to answers, a seamless flow,\n",
      "Unlocking insights, where knowledge can grow.\n",
      "\n",
      "So let us sing, of LangChain's might,\n",
      "A beacon of knowledge, shining bright.\n",
      "In the world of AI, it's a rising star,\n",
      "LangChain, the future, near and far. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Gemini\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "os.environ['GOOGLE_API_KEY'] =os.getenv('GOOGLE_API_KEY')\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n",
    "result = llm.invoke(\"Write a ballad about LangChain\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23a876da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Sure, I\\'d be happy to explain!\\n\\nLLM stands for \"Low Latency Logging,\" which is a method of recording and transmitting data with minimal delay. Low latency LLMs are particularly important in time-sensitive applications where real-time data processing is critical. Here are some reasons why low latency LLMs are important:\\n\\n1. Improved user experience: In applications such as online gaming or video conferencing, low latency LLMs ensure that data is transmitted and processed quickly, resulting in a smoother and more responsive user experience.\\n2. Increased efficiency: Low latency LLMs enable faster data processing, which can lead to increased efficiency in data-intensive applications such as financial trading or scientific simulations.\\n3. Enhanced security: In security-critical applications, low latency LLMs can help detect and respond to threats more quickly, reducing the risk of data breaches or other security incidents.\\n4. Better decision-making: In real-time decision-making scenarios, low latency LLMs can provide decision-makers with up-to-the-minute data, enabling them to make more informed and timely decisions.\\n5. Improved reliability: Low latency LLMs can help ensure that data is transmitted and processed consistently and reliably, reducing the risk of data loss or corruption.\\n\\nOverall, low latency LLMs are essential in applications where real-time data processing is critical. By minimizing delay and ensuring fast and reliable data transmission, low latency LLMs can help improve user experience, increase efficiency, enhance security, enable better decision-making, and improve reliability.', response_metadata={'token_usage': {'completion_time': 0.610208635, 'completion_tokens': 348, 'prompt_time': 0.006191451, 'prompt_tokens': 26, 'queue_time': None, 'total_time': 0.616400086, 'total_tokens': 374}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-9702b101-ef22-4dc7-9ff1-00ba3b2af6a1-0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mistral 8*7b\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "load_dotenv()\n",
    "chat = ChatGroq(temperature=0, groq_api_key=os.getenv(\"GROQ_API_KEY\"), model_name=\"mixtral-8x7b-32768\")\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06857803",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Low-latency Large Language Models (LLMs) are crucial in various applications where real-time or near-real-time processing is essential. Here are some reasons why low-latency LLMs are important:\\n\\n1. **Conversational AI**: In conversational AI, low-latency LLMs enable more natural and human-like interactions. Fast response times are critical in chatbots, virtual assistants, and voice assistants to maintain user engagement and provide a seamless experience.\\n2. **Real-time Language Translation**: Low-latency LLMs facilitate instant language translation, which is vital in applications like live subtitles, real-time translation for international conferences, or simultaneous interpretation for diplomatic meetings.\\n3. **Sentiment Analysis and Feedback**: In customer service, low-latency LLMs can quickly analyze customer feedback, enabling companies to respond promptly to concerns and improve customer satisfaction.\\n4. **Content Generation and Summarization**: Fast LLMs can generate content, such as news summaries or product descriptions, in real-time, allowing for timely and relevant information dissemination.\\n5. **Gaming and Interactive Systems**: Low-latency LLMs can enhance gaming experiences by enabling rapid language understanding and generation, creating more immersive and interactive environments.\\n6. **Healthcare and Emergency Services**: In healthcare, low-latency LLMs can quickly analyze medical texts, enabling faster diagnosis and treatment. In emergency services, rapid language processing can facilitate swift response times and save lives.\\n7. **Autonomous Systems**: In autonomous vehicles, drones, or robots, low-latency LLMs can process and respond to voice commands, enabling more efficient and safe operation.\\n8. **Edge AI and IoT**: With the proliferation of edge AI and IoT devices, low-latency LLMs can process and analyze data in real-time, reducing latency and improving overall system performance.\\n9. **Cybersecurity**: Fast LLMs can quickly analyze network traffic, detect anomalies, and respond to cyber threats, helping to prevent data breaches and protect sensitive information.\\n10. **Competitive Advantage**: In many industries, low-latency LLMs can provide a competitive edge by enabling faster decision-making, improving customer experiences, and increasing operational efficiency.\\n\\nIn summary, low-latency LLMs are essential for applications that require rapid language processing, analysis, and generation. They can significantly improve user experiences, enhance decision-making, and provide a competitive advantage in various industries.', response_metadata={'token_usage': {'completion_time': 1.575, 'completion_tokens': 484, 'prompt_time': 0.019, 'prompt_tokens': 32, 'queue_time': None, 'total_time': 1.5939999999999999, 'total_tokens': 516}, 'model_name': 'llama3-70b-8192', 'system_fingerprint': 'fp_87cbfbbc4d', 'finish_reason': 'stop', 'logprobs': None}, id='run-69cdf529-f016-4cb0-b496-69e32596db13-0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LLama3 70 b\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "chat = ChatGroq(temperature=0, groq_api_key=os.getenv(\"GROQ_API_KEY\"), model_name='llama3-70b-8192')\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "599fdf13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Large Language Models (LLMs) have revolutionized the field of natural language processing (NLP) by enabling applications such as language translation, text summarization, and chatbots. However, traditional LLMs often suffer from high latency, which can be a significant limitation in many real-world applications. Low latency LLMs, on the other hand, offer several advantages that make them crucial for various use cases. Here are some reasons why low latency LLMs are important:\\n\\n1. **Real-time processing**: Low latency LLMs enable real-time processing of user input, which is essential for applications like chatbots, virtual assistants, and live language translation. This allows for a more seamless and responsive user experience.\\n2. **Improved user engagement**: Fast response times can significantly improve user engagement and satisfaction. Low latency LLMs can process user input quickly, reducing the likelihood of user frustration and abandonment.\\n3. **Enhanced decision-making**: In applications like customer service or technical support, low latency LLMs can provide instant responses to user queries, enabling faster issue resolution and improved customer satisfaction.\\n4. **Increased efficiency**: Low latency LLMs can automate routine tasks, freeing up human agents to focus on more complex and high-value tasks. This can lead to increased efficiency and reduced operational costs.\\n5. **Competitive advantage**: In industries like finance, healthcare, or e-commerce, low latency LLMs can provide a competitive advantage by enabling faster and more accurate processing of user input, leading to improved customer experiences and increased loyalty.\\n6. **Scalability**: Low latency LLMs can be designed to scale horizontally, allowing them to handle a large volume of user requests without compromising performance. This makes them suitable for large-scale applications.\\n7. **Improved accuracy**: Low latency LLMs can leverage real-time feedback to improve their accuracy and adapt to changing user behavior, leading to better overall performance and more accurate responses.\\n8. **Enhanced security**: Low latency LLMs can be designed with security in mind, enabling real-time threat detection and response to potential security breaches.\\n9. **New use cases**: Low latency LLMs can enable new use cases that rely on real-time processing, such as:\\n\\t* Real-time language translation for international communication\\n\\t* Instant text summarization for news and research\\n\\t* Real-time sentiment analysis for customer feedback\\n10. **Future-proofing**: As the demand for real-time processing continues to grow, low latency LLMs can future-proof applications and ensure they remain competitive in the market.\\n\\nIn summary, low latency LLMs are crucial for applications that require real-time processing, improved user engagement, and enhanced decision-making. They offer a competitive advantage, improved accuracy, and scalability, making them an essential component of many industries and applications.', response_metadata={'token_usage': {'completion_time': 0.665, 'completion_tokens': 565, 'prompt_time': 0.013, 'prompt_tokens': 32, 'queue_time': None, 'total_time': 0.678, 'total_tokens': 597}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_a97cfe35ae', 'finish_reason': 'stop', 'logprobs': None}, id='run-e80b8c40-37a4-4f22-9316-06eaf09f41ca-0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#llama3 8b\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_groq import ChatGroq\n",
    "chat = ChatGroq(temperature=0, groq_api_key=os.getenv(\"GROQ_API_KEY\"), model_name='llama3-8b-8192')\n",
    "system = \"You are a helpful assistant.\"\n",
    "human = \"{text}\"\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system), (\"human\", human)])\n",
    "\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"text\": \"Explain the importance of low latency LLMs.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a326e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='What do you call a bear with no teeth? A gummy bear!', additional_kwargs={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'd43d7404-e608-4c47-bb97-afb7895f549a', 'token_count': {'input_tokens': 72, 'output_tokens': 14}}, response_metadata={'documents': None, 'citations': None, 'search_results': None, 'search_queries': None, 'is_search_required': None, 'generation_id': 'd43d7404-e608-4c47-bb97-afb7895f549a', 'token_count': {'input_tokens': 72, 'output_tokens': 14}}, id='run-9143b017-bc26-4fd1-b4f4-b2db179b08c1-0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ['COHERE_API_KEY'] = os.getenv('COHERE_API_KEY')\n",
    "from langchain_cohere import ChatCohere\n",
    "chat = ChatCohere(model=\"command-r\")\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = prompt | chat\n",
    "chain.invoke({\"topic\": \"bears\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6002947f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a758b72a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
